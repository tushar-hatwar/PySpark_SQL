{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import row_number, rank, dense_rank\nfrom pyspark.sql.window import Window\ndata = [('Anil','HR',2000),('Arun','IT',3000),('Sandeep','HR',1500),\\\n    ('Annu','payroll',3500),('Shakti','IT',3000),('pradeep','IT',4000),\\\n    ('Kartik','payroll',2000),('Himanshu','IT',2000),('Bhargav','HR',2000),\\\n    ('Morthi','IT',2500)]\nschema = ['name','dep','salary']\ndf = spark.createDataFrame(data, schema)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d22eae4c-7d6e-4075-9fb4-46ad9f82b20c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------+-------+------+\n|    name|    dep|salary|\n+--------+-------+------+\n|    Anil|     HR|  2000|\n|    Arun|     IT|  3000|\n| Sandeep|     HR|  1500|\n|    Annu|payroll|  3500|\n|  Shakti|     IT|  3000|\n| pradeep|     IT|  4000|\n|  Kartik|payroll|  2000|\n|Himanshu|     IT|  2000|\n| Bhargav|     HR|  2000|\n|  Morthi|     IT|  2500|\n+--------+-------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#calculate total salary taken by a department\ndf.groupBy(\"dep\").sum(\"salary\") \\\n.withColumnRenamed(\"sum(salary)\", \"Department_Take\") \\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"13bb2293-8897-429d-808d-bb526f4d48ca","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+---------------+\n|dep    |Department_Take|\n+-------+---------------+\n|HR     |5500           |\n|IT     |14500          |\n|payroll|5500           |\n+-------+---------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col\n#USE OF HAVING CLAUSE - using where or filter\ndf.groupBy(\"dep\").count() \\\n.withColumnRenamed(\"count\", \"Total_Employees\") \\\n.filter(col(\"Total_Employees\")>2) \\\n.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7075d614-5694-419c-b570-e4557c5f9fa1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---------------+\n|dep|Total_Employees|\n+---+---------------+\n|HR |3              |\n|IT |5              |\n+---+---------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import sum,avg,max, round\ndf.groupBy(\"dep\") \\\n    .agg(sum(\"salary\").alias(\"sum_salary\"), \\\n         (round(avg(\"salary\"),2)).alias(\"avg_salary\")) \\\n    .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a5d2dc07-2407-4cd7-a702-0a91f75c6250","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+----------+----------+\n|dep    |sum_salary|avg_salary|\n+-------+----------+----------+\n|HR     |5500      |1833.33   |\n|IT     |14500     |2900.0    |\n|payroll|5500      |2750.0    |\n+-------+----------+----------+\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Group By","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":249263314083118}},"nbformat":4,"nbformat_minor":0}
